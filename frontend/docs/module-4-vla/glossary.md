# Common Terminology and Glossary for VLA Module

## Key Terms

### Vision-Language-Action (VLA)
A system architecture that combines computer vision, natural language processing, and robotic action planning to enable intelligent robot behavior based on visual and linguistic inputs.

### Voice Command
A natural language instruction provided by a user that contains intent and parameters for robot action execution.

### Transcribed Text
The text representation of a voice command after speech-to-text processing, which serves as input for intent extraction.

### Intent Structure
A parsed representation of command intent with parameters for action execution, derived from transcribed text.

### ROS 2 Action
A standardized command format published to ROS 2 topics for robot control, representing a specific task or behavior.

### LLM Plan
A sequence of actions generated by a large language model to achieve high-level goals, incorporating task decomposition and planning.

### Constraint Set
Safety and operational boundaries that all robot actions must satisfy before execution, ensuring safe and appropriate behavior.

### VLA Pipeline
An integrated system that combines voice processing, language understanding, perception, and control components to enable autonomous behavior.

### Speech-to-Text (STT)
The process of converting spoken language into written text, typically using models like OpenAI Whisper.

### Intent Extraction
The process of identifying the user's intent and relevant parameters from transcribed text or direct text input.

### Constraint-Aware Planning
Planning that incorporates safety and operational constraints to ensure generated action sequences are safe and feasible.

### Human-in-the-Loop Control
A system design that allows human operators to intervene, guide, or override autonomous robot behavior when necessary.

### Simulation Validation
The process of testing and validating VLA systems in simulation environments before deployment to physical robots.